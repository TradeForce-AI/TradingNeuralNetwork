{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPLtuK82YrPP3sZX8Ljab45",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TradeForce-AI/TradingNeuralNetwork/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxgBQq3zq4EE"
      },
      "source": [
        "!pip install yfinance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G78pSqGpqYxL"
      },
      "source": [
        "import pandas as pd\r\n",
        "import math\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras import Input\r\n",
        "from keras.layers import Dropout\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "import yfinance as yf\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-GINQTnqn02",
        "outputId": "0b34a1f8-6242-455b-ccd6-fd92e34ad423"
      },
      "source": [
        "instrument = 'AAPL'\r\n",
        "data = yf.download(instrument, start='1990-01-01', interval=\"1d\")\r\n",
        "# Calculate the point difference \r\n",
        "data['Points'] = data['Adj Close'] - data['Open']\r\n",
        "\r\n",
        "# Shift the Point column up (This will be our Target label)\r\n",
        "data['Points'] = data['Points'].shift(periods=-1)\r\n",
        "\r\n",
        "# Calculate the single bar range\r\n",
        "data['Bar_Range'] = data.High - data.Low\r\n",
        "\r\n",
        "\r\n",
        "data.dropna(inplace=True)\r\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 7854 entries, 1990-01-02 to 2021-03-04\n",
            "Data columns (total 8 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   Open       7854 non-null   float64\n",
            " 1   High       7854 non-null   float64\n",
            " 2   Low        7854 non-null   float64\n",
            " 3   Close      7854 non-null   float64\n",
            " 4   Adj Close  7854 non-null   float64\n",
            " 5   Volume     7854 non-null   int64  \n",
            " 6   Points     7854 non-null   float64\n",
            " 7   Bar_Range  7854 non-null   float64\n",
            "dtypes: float64(7), int64(1)\n",
            "memory usage: 552.2 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "7GZ2n2h9rPnP",
        "outputId": "3a544248-4c6d-4a39-e902-c1d8d0bb0ca3"
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Points</th>\n",
              "      <th>Bar_Range</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2021-02-26</th>\n",
              "      <td>122.589996</td>\n",
              "      <td>124.849998</td>\n",
              "      <td>121.199997</td>\n",
              "      <td>121.260002</td>\n",
              "      <td>121.260002</td>\n",
              "      <td>164320000</td>\n",
              "      <td>4.040001</td>\n",
              "      <td>3.650002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-01</th>\n",
              "      <td>123.750000</td>\n",
              "      <td>127.930000</td>\n",
              "      <td>122.790001</td>\n",
              "      <td>127.790001</td>\n",
              "      <td>127.790001</td>\n",
              "      <td>115998300</td>\n",
              "      <td>-3.290001</td>\n",
              "      <td>5.139999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-02</th>\n",
              "      <td>128.410004</td>\n",
              "      <td>128.720001</td>\n",
              "      <td>125.010002</td>\n",
              "      <td>125.120003</td>\n",
              "      <td>125.120003</td>\n",
              "      <td>102015300</td>\n",
              "      <td>-2.750000</td>\n",
              "      <td>3.709999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-03</th>\n",
              "      <td>124.809998</td>\n",
              "      <td>125.709999</td>\n",
              "      <td>121.839996</td>\n",
              "      <td>122.059998</td>\n",
              "      <td>122.059998</td>\n",
              "      <td>112430400</td>\n",
              "      <td>-1.620003</td>\n",
              "      <td>3.870003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-04</th>\n",
              "      <td>121.750000</td>\n",
              "      <td>123.599998</td>\n",
              "      <td>118.620003</td>\n",
              "      <td>120.129997</td>\n",
              "      <td>120.129997</td>\n",
              "      <td>177275300</td>\n",
              "      <td>-0.660004</td>\n",
              "      <td>4.979996</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Open        High         Low  ...     Volume    Points  Bar_Range\n",
              "Date                                            ...                                \n",
              "2021-02-26  122.589996  124.849998  121.199997  ...  164320000  4.040001   3.650002\n",
              "2021-03-01  123.750000  127.930000  122.790001  ...  115998300 -3.290001   5.139999\n",
              "2021-03-02  128.410004  128.720001  125.010002  ...  102015300 -2.750000   3.709999\n",
              "2021-03-03  124.809998  125.709999  121.839996  ...  112430400 -1.620003   3.870003\n",
              "2021-03-04  121.750000  123.599998  118.620003  ...  177275300 -0.660004   4.979996\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ixdn2S44nIk"
      },
      "source": [
        "## Vanilla LSTM\r\n",
        "\r\n",
        "A Vanilla LSTM is an LSTM model that has a single hidden layer of LSTM units, and an output layer used to make a prediction.\r\n",
        "\r\n",
        "In this case, we define a model with 50 LSTM units in the hidden layer and an output layer that predicts a single numerical value.\r\n",
        "\r\n",
        "The model is fit using the efficient Adam version of stochastic gradient descent and optimized using the mean squared error, or ‘mse‘ loss function.\r\n",
        "\r\n",
        "Once the model is defined, we can fit it on the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c8Hn45aU7Abr"
      },
      "source": [
        "# univariate lstm example\r\n",
        "from numpy import array\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Dense\r\n",
        "\r\n",
        "# split a univariate sequence into samples\r\n",
        "def split_sequence(sequence, n_steps):\r\n",
        "\tX, y = list(), list()\r\n",
        "\tfor i in range(len(sequence)):\r\n",
        "\t\t# find the end of this pattern\r\n",
        "\t\tend_ix = i + n_steps\r\n",
        "\t\t# check if we are beyond the sequence\r\n",
        "\t\tif end_ix > len(sequence)-1:\r\n",
        "\t\t\tbreak\r\n",
        "\t\t# gather input and output parts of the pattern\r\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\r\n",
        "\t\tX.append(seq_x)\r\n",
        "\t\ty.append(seq_y)\r\n",
        "\treturn array(X), array(y)\r\n",
        "\r\n",
        "# define input sequence\r\n",
        "raw_seq = data['Adj Close']\r\n",
        "\r\n",
        "# choose a number of time steps\r\n",
        "n_steps = 3\r\n",
        "\r\n",
        "# split into samples\r\n",
        "X, y = split_sequence(raw_seq, n_steps)\r\n",
        "\r\n",
        "# reshape from [samples, timesteps] into [samples, timesteps, features]\r\n",
        "n_features = 1\r\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\r\n",
        "\r\n",
        "# define model\r\n",
        "model = Sequential()\r\n",
        "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\r\n",
        "model.add(Dense(1))\r\n",
        "model.compile(optimizer='adam', loss='mse')\r\n",
        "# fit model\r\n",
        "model.fit(X, y, epochs=200, verbose=1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leYlfwGK-1j1"
      },
      "source": [
        "To make a prediction ,we must pass as input the last 3 Close prices.\r\n",
        "The model expects the input shape to be three-dimensional with [samples,timesteps, features], therefore, we must reshape the sample before making the prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebuA_KBz74ez",
        "outputId": "7211c935-95cd-4b91-f981-b2bdc7601904"
      },
      "source": [
        "# demonstrate prediction\r\n",
        "\r\n",
        "x_input = pd.Series(data['Adj Close'].iloc[-3:])\r\n",
        "x_input = x_input.values.reshape((1, n_steps, n_features))\r\n",
        "yhat = model.predict(x_input, verbose=1)\r\n",
        "print(yhat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 128ms/step\n",
            "[[122.71884]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NV6C6DRDPA1"
      },
      "source": [
        "## Stacked LSTM\r\n",
        "\r\n",
        "Multiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model.\r\n",
        "\r\n",
        "An LSTM layer requires a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence.\r\n",
        "\r\n",
        "We can address this by having the LSTM output a value for each time step in the input data by setting the return_sequences=True argument on the layer. This allows us to have 3D output from hidden LSTM layer as input to the next.\r\n",
        "\r\n",
        "We can therefore define a Stacked LSTM as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVjTbKpdBoIa"
      },
      "source": [
        "# univariate stacked lstm example\r\n",
        "from numpy import array\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Dense\r\n",
        "\r\n",
        "# split a univariate sequence\r\n",
        "def split_sequence(sequence, n_steps):\r\n",
        "\tX, y = list(), list()\r\n",
        "\tfor i in range(len(sequence)):\r\n",
        "\t\t# find the end of this pattern\r\n",
        "\t\tend_ix = i + n_steps\r\n",
        "\t\t# check if we are beyond the sequence\r\n",
        "\t\tif end_ix > len(sequence)-1:\r\n",
        "\t\t\tbreak\r\n",
        "\t\t# gather input and output parts of the pattern\r\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\r\n",
        "\t\tX.append(seq_x)\r\n",
        "\t\ty.append(seq_y)\r\n",
        "\treturn array(X), array(y)\r\n",
        "\r\n",
        "# define input sequence\r\n",
        "raw_seq = data['Adj Close']\r\n",
        "# choose a number of time steps\r\n",
        "n_steps = 5\r\n",
        "# split into samples\r\n",
        "X, y = split_sequence(raw_seq, n_steps)\r\n",
        "# reshape from [samples, timesteps] into [samples, timesteps, features]\r\n",
        "n_features = 1\r\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\r\n",
        "# define model\r\n",
        "model = Sequential()\r\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\r\n",
        "model.add(LSTM(50, activation='relu'))\r\n",
        "model.add(Dense(1))\r\n",
        "model.compile(optimizer='adam', loss='mse')\r\n",
        "# fit model\r\n",
        "model.fit(X, y, epochs=200, verbose=1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3E21LB3F04w"
      },
      "source": [
        "To make a prediction ,we must pass as input the last 3 Close prices. The model expects the input shape to be three-dimensional with [samples,timesteps, features], therefore, we must reshape the sample before making the prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVCvVypBDjIq",
        "outputId": "bb527331-2de2-43dc-9c4b-ca6ad69a7ad7"
      },
      "source": [
        "# demonstrate prediction\r\n",
        "x_input = pd.Series(data['Adj Close'].iloc[-3:])\r\n",
        "x_input = x_input.values.reshape((1, n_steps, n_features))\r\n",
        "yhat = model.predict(x_input, verbose=0)\r\n",
        "print(yhat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[121.3201]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_qnTtwSGDSk"
      },
      "source": [
        "## Bidirectional LSTM\r\n",
        "\r\n",
        "On some sequence prediction problems, it can be beneficial to allow the LSTM model to learn the input sequence both forward and backwards and concatenate both interpretations.\r\n",
        "\r\n",
        "This is called a Bidirectional LSTM.\r\n",
        "\r\n",
        "We can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional.\r\n",
        "\r\n",
        "An example of defining a Bidirectional LSTM to read input both forward and backward is as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riWmmjB8F3eq"
      },
      "source": [
        "# univariate bidirectional lstm example\r\n",
        "from numpy import array\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Bidirectional\r\n",
        "\r\n",
        "# split a univariate sequence\r\n",
        "def split_sequence(sequence, n_steps):\r\n",
        "\tX, y = list(), list()\r\n",
        "\tfor i in range(len(sequence)):\r\n",
        "\t\t# find the end of this pattern\r\n",
        "\t\tend_ix = i + n_steps\r\n",
        "\t\t# check if we are beyond the sequence\r\n",
        "\t\tif end_ix > len(sequence)-1:\r\n",
        "\t\t\tbreak\r\n",
        "\t\t# gather input and output parts of the pattern\r\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\r\n",
        "\t\tX.append(seq_x)\r\n",
        "\t\ty.append(seq_y)\r\n",
        "\treturn array(X), array(y)\r\n",
        "\r\n",
        "# define input sequence\r\n",
        "raw_seq = data['Adj Close']\r\n",
        "# choose a number of time steps\r\n",
        "n_steps = 3\r\n",
        "# split into samples\r\n",
        "X, y = split_sequence(raw_seq, n_steps)\r\n",
        "# reshape from [samples, timesteps] into [samples, timesteps, features]\r\n",
        "n_features = 1\r\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\r\n",
        "# define model\r\n",
        "model = Sequential()\r\n",
        "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\r\n",
        "model.add(Dense(1))\r\n",
        "model.compile(optimizer='adam', loss='mse')\r\n",
        "# fit model\r\n",
        "model.fit(X, y, epochs=200, verbose=1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPKi1L99GRu7",
        "outputId": "53b8ac0b-ec97-469e-82ee-f527191e67a0"
      },
      "source": [
        "# demonstrate prediction\r\n",
        "x_input = pd.Series(data['Adj Close'].iloc[-3:])\r\n",
        "x_input = x_input.values.reshape((1, n_steps, n_features))\r\n",
        "yhat = model.predict(x_input, verbose=0)\r\n",
        "print(yhat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[123.53392]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkibsLztGvdH"
      },
      "source": [
        "## CNN LSTM\r\n",
        "A convolutional neural network, or CNN for short, is a type of neural network developed for working with two-dimensional image data.\r\n",
        "\r\n",
        "The CNN can be very effective at automatically extracting and learning features from one-dimensional sequence data such as univariate time series data.\r\n",
        "\r\n",
        "A CNN model can be used in a hybrid model with an LSTM backend where the CNN is used to interpret subsequences of input that together are provided as a sequence to an LSTM model to interpret. This hybrid model is called a CNN-LSTM.\r\n",
        "\r\n",
        "The first step is to split the input sequences into subsequences that can be processed by the CNN model. For example, we can first split our univariate time series data into input/output samples with four steps as input and one as output. Each sample can then be split into two sub-samples, each with two time steps. The CNN can interpret each subsequence of two time steps and provide a time series of interpretations of the subsequences to the LSTM model to process as input.\r\n",
        "\r\n",
        "We can parameterize this and define the number of subsequences as n_seq and the number of time steps per subsequence as n_steps. The input data can then be reshaped to have the required structure:\r\n",
        "\r\n",
        "[samples, subsequences, timesteps, features]\r\n",
        "\r\n",
        "\r\n",
        "We want to reuse the same CNN model when reading in each sub-sequence of data separately.\r\n",
        "\r\n",
        "This can be achieved by wrapping the entire CNN model in a TimeDistributed wrapper that will apply the entire model once per input, in this case, once per input subsequence.\r\n",
        "\r\n",
        "The CNN model first has a convolutional layer for reading across the subsequence that requires a number of filters and a kernel size to be specified. The number of filters is the number of reads or interpretations of the input sequence. The kernel size is the number of time steps included of each ‘read’ operation of the input sequence.\r\n",
        "\r\n",
        "The convolution layer is followed by a max pooling layer that distills the filter maps down to 1/2 of their size that includes the most salient features. These structures are then flattened down to a single one-dimensional vector to be used as a single input time step to the LSTM layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B23bw1_7IHi9"
      },
      "source": [
        "# univariate cnn lstm example\r\n",
        "from numpy import array\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import TimeDistributed\r\n",
        "from keras.layers.convolutional import Conv1D\r\n",
        "from keras.layers.convolutional import MaxPooling1D\r\n",
        "\r\n",
        "# split a univariate sequence into samples\r\n",
        "def split_sequence(sequence, n_steps):\r\n",
        "\tX, y = list(), list()\r\n",
        "\tfor i in range(len(sequence)):\r\n",
        "\t\t# find the end of this pattern\r\n",
        "\t\tend_ix = i + n_steps\r\n",
        "\t\t# check if we are beyond the sequence\r\n",
        "\t\tif end_ix > len(sequence)-1:\r\n",
        "\t\t\tbreak\r\n",
        "\t\t# gather input and output parts of the pattern\r\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\r\n",
        "\t\tX.append(seq_x)\r\n",
        "\t\ty.append(seq_y)\r\n",
        "\treturn array(X), array(y)\r\n",
        "\r\n",
        "# define input sequence\r\n",
        "raw_seq = data['Adj Close']\r\n",
        "# choose a number of time steps\r\n",
        "n_steps = 4\r\n",
        "# split into samples\r\n",
        "X, y = split_sequence(raw_seq, n_steps)\r\n",
        "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\r\n",
        "n_features = 1\r\n",
        "n_seq = 2\r\n",
        "n_steps = 2\r\n",
        "X = X.reshape((X.shape[0], n_seq, n_steps, n_features))\r\n",
        "# define model\r\n",
        "model = Sequential()\r\n",
        "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\r\n",
        "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\r\n",
        "model.add(TimeDistributed(Flatten()))\r\n",
        "model.add(LSTM(50, activation='relu'))\r\n",
        "model.add(Dense(1))\r\n",
        "model.compile(optimizer='adam', loss='mse')\r\n",
        "# fit model\r\n",
        "model.fit(X, y, epochs=500, verbose=1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq4XIOR8Ib-U",
        "outputId": "ab4da10c-d537-48d0-dfec-8525f1cbe91e"
      },
      "source": [
        "# demonstrate prediction\r\n",
        "x_input = pd.Series(data['Adj Close'].iloc[-4:])\r\n",
        "x_input = x_input.values.reshape((1, n_seq, n_steps, n_features))\r\n",
        "yhat = model.predict(x_input, verbose=0)\r\n",
        "print(yhat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 249 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0f499edef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[[124.05951]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eez9sTJJ3vn"
      },
      "source": [
        "## ConvLSTM\r\n",
        "A type of LSTM related to the CNN-LSTM is the ConvLSTM, where the convolutional reading of input is built directly into each LSTM unit.\r\n",
        "\r\n",
        "The ConvLSTM was developed for reading two-dimensional spatial-temporal data, but can be adapted for use with univariate time series forecasting.\r\n",
        "\r\n",
        "The layer expects input as a sequence of two-dimensional images, therefore the shape of input data must be:\r\n",
        "\r\n",
        "[samples, timesteps, rows, columns, features]\r\n",
        "\r\n",
        "For our purposes, we can split each sample into subsequences where timesteps will become the number of subsequences, or n_seq, and columns will be the number of time steps for each subsequence, or n_steps. The number of rows is fixed at 1 as we are working with one-dimensional data.\r\n",
        "\r\n",
        "We can define the ConvLSTM as a single layer in terms of the number of filters and a two-dimensional kernel size in terms of (rows, columns). As we are working with a one-dimensional series, the number of rows is always fixed to 1 in the kernel.\r\n",
        "\r\n",
        "The output of the model must then be flattened before it can be interpreted and a prediction made."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awc6dgSSJ-IA"
      },
      "source": [
        "# univariate convlstm example\r\n",
        "from numpy import array\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import ConvLSTM2D\r\n",
        "\r\n",
        "# split a univariate sequence into samples\r\n",
        "def split_sequence(sequence, n_steps):\r\n",
        "\tX, y = list(), list()\r\n",
        "\tfor i in range(len(sequence)):\r\n",
        "\t\t# find the end of this pattern\r\n",
        "\t\tend_ix = i + n_steps\r\n",
        "\t\t# check if we are beyond the sequence\r\n",
        "\t\tif end_ix > len(sequence)-1:\r\n",
        "\t\t\tbreak\r\n",
        "\t\t# gather input and output parts of the pattern\r\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\r\n",
        "\t\tX.append(seq_x)\r\n",
        "\t\ty.append(seq_y)\r\n",
        "\treturn array(X), array(y)\r\n",
        "\r\n",
        "# define input sequence\r\n",
        "raw_seq = data['Adj Close']\r\n",
        "# choose a number of time steps\r\n",
        "n_steps = 4\r\n",
        "# split into samples\r\n",
        "X, y = split_sequence(raw_seq, n_steps)\r\n",
        "# reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features]\r\n",
        "n_features = 1\r\n",
        "n_seq = 2\r\n",
        "n_steps = 2\r\n",
        "X = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))\r\n",
        "# define model\r\n",
        "model = Sequential()\r\n",
        "model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(1))\r\n",
        "model.compile(optimizer='adam', loss='mse')\r\n",
        "# fit model\r\n",
        "model.fit(X, y, epochs=500, verbose=1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1Sj3cq6KhyF",
        "outputId": "d9563c87-5c82-455c-cf4c-66e61eb829f6"
      },
      "source": [
        "# demonstrate prediction\r\n",
        "x_input = pd.Series(data['Adj Close'].iloc[-4:])\r\n",
        "x_input = x_input.values.reshape((1, n_seq, 1, n_steps, n_features))\r\n",
        "yhat = model.predict(x_input, verbose=1)\r\n",
        "print(yhat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 250 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0f4b359050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[[123.65139]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnKQysTwqqxF"
      },
      "source": [
        "## Multivariate LSTM Models\r\n",
        "\r\n",
        "Multivariate time series data means data where there is more than one observation for each time step.\r\n",
        "\r\n",
        "There are two main models that we may require with multivariate time series data; they are:\r\n",
        "\r\n",
        "*   **Multiple Input Series.**\r\n",
        "*   **Multiple Parallel Series.**\r\n",
        "\r\n",
        "Multiple Input Series\r\n",
        "A problem may have two or more parallel input time series and an output time series that is dependent on the input time series.\r\n",
        "\r\n",
        "The input time series are parallel because each series has an observation at the same time steps.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQTNdTxorKB3"
      },
      "source": [
        "# multivariate lstm example\r\n",
        "from numpy import array\r\n",
        "from numpy import hstack\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Dense\r\n",
        " \r\n",
        "# split a multivariate sequence into samples\r\n",
        "def split_sequences(sequences, n_steps):\r\n",
        "\tX, y = list(), list()\r\n",
        "\tfor i in range(len(sequences)):\r\n",
        "\t\t# find the end of this pattern\r\n",
        "\t\tend_ix = i + n_steps\r\n",
        "\t\t# check if we are beyond the dataset\r\n",
        "\t\tif end_ix > len(sequences):\r\n",
        "\t\t\tbreak\r\n",
        "\t\t# gather input and output parts of the pattern\r\n",
        "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\r\n",
        "\t\tX.append(seq_x)\r\n",
        "\t\ty.append(seq_y)\r\n",
        "\treturn array(X), array(y)\r\n",
        " \r\n",
        "# define input sequence\r\n",
        "in_seq1 = data['Adj Close']\r\n",
        "in_seq2 = data['Bar_Range']\r\n",
        "out_seq = data['Points']\r\n",
        "# convert to [rows, columns] structure\r\n",
        "in_seq1 = in_seq1.values.reshape((len(in_seq1), 1))\r\n",
        "in_seq2 = in_seq2.values.reshape((len(in_seq2), 1))\r\n",
        "out_seq = out_seq.values.reshape((len(out_seq), 1))\r\n",
        "# horizontally stack columns\r\n",
        "dataset = hstack((in_seq1, in_seq2, out_seq))\r\n",
        "# choose a number of time steps\r\n",
        "n_steps = 3\r\n",
        "# convert into input/output\r\n",
        "X, y = split_sequences(dataset, n_steps)\r\n",
        "# the dataset knows the number of features, e.g. 2\r\n",
        "n_features = X.shape[2]\r\n",
        "# define model\r\n",
        "model = Sequential()\r\n",
        "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\r\n",
        "model.add(Dense(1))\r\n",
        "model.compile(optimizer='adam', loss='mse')\r\n",
        "# fit model\r\n",
        "model.fit(X, y, epochs=200, verbose=1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPTA6pnIuRuP",
        "outputId": "4d16082e-ab13-4a59-f203-3470a71b508e"
      },
      "source": [
        "# demonstrate prediction\r\n",
        "seq1 = data[['Adj Close','Bar_Range']][-4:-3]\r\n",
        "seq2 = data[['Adj Close','Bar_Range']][-3:-2]\r\n",
        "seq3 = data[['Adj Close','Bar_Range']][-2:-1]\r\n",
        "\r\n",
        "x_input = array([seq1, seq2, seq3])\r\n",
        "x_input = x_input.reshape((1, n_steps, n_features))\r\n",
        "yhat = model.predict(x_input, verbose=0)\r\n",
        "print(f'Predicted Points: {yhat}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Points: [[-1.1422715]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE8CXvCKtqwL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knUrIHUs7u3y"
      },
      "source": [
        "## Encoder-Decoder Model\r\n",
        "A model specifically developed for forecasting variable length output sequences is called the Encoder-Decoder LSTM.\r\n",
        "\r\n",
        "The model was designed for prediction problems where there are both input and output sequences, so-called sequence-to-sequence, or seq2seq problems, such as translating text from one language to another.\r\n",
        "\r\n",
        "This model can be used for multi-step time series forecasting.\r\n",
        "\r\n",
        "As its name suggests, the model is comprised of two sub-models: the encoder and the decoder.\r\n",
        "\r\n",
        "The encoder is a model responsible for reading and interpreting the input sequence. The output of the encoder is a fixed length vector that represents the model’s interpretation of the sequence. The encoder is traditionally a Vanilla LSTM model, although other encoder models can be used such as Stacked, Bidirectional, and CNN models.\r\n",
        "\r\n",
        "\r\n",
        "The decoder uses the output of the encoder as an input.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ohU9unn8FgV",
        "outputId": "2e8bca81-ec65-4b8a-f2f3-d34b72eb50ac"
      },
      "source": [
        "# univariate multi-step encoder-decoder lstm example\r\n",
        "from numpy import array\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import RepeatVector\r\n",
        "from keras.layers import TimeDistributed\r\n",
        "\r\n",
        "# split a univariate sequence into samples\r\n",
        "def split_sequence(sequence, n_steps_in, n_steps_out):\r\n",
        "\tX, y = list(), list()\r\n",
        "\tfor i in range(len(sequence)):\r\n",
        "\t\t# find the end of this pattern\r\n",
        "\t\tend_ix = i + n_steps_in\r\n",
        "\t\tout_end_ix = end_ix + n_steps_out\r\n",
        "\t\t# check if we are beyond the sequence\r\n",
        "\t\tif out_end_ix > len(sequence):\r\n",
        "\t\t\tbreak\r\n",
        "\t\t# gather input and output parts of the pattern\r\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\r\n",
        "\t\tX.append(seq_x)\r\n",
        "\t\ty.append(seq_y)\r\n",
        "\treturn array(X), array(y)\r\n",
        "\r\n",
        "# define input sequence\r\n",
        "raw_seq = data['Adj Close']\r\n",
        "# choose a number of time steps\r\n",
        "n_steps_in, n_steps_out = 3, 1\r\n",
        "# split into samples\r\n",
        "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\r\n",
        "# reshape from [samples, timesteps] into [samples, timesteps, features]\r\n",
        "n_features = 1\r\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\r\n",
        "y = y.reshape((y.shape[0], y.shape[1], n_features))\r\n",
        "# define model\r\n",
        "model = Sequential()\r\n",
        "model.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features)))\r\n",
        "model.add(RepeatVector(n_steps_out))\r\n",
        "model.add(LSTM(100, activation='relu', return_sequences=True))\r\n",
        "model.add(TimeDistributed(Dense(1)))\r\n",
        "model.compile(optimizer='adam', loss='mse')\r\n",
        "# fit model\r\n",
        "model.fit(X, y, epochs=100, verbose=1)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/100\n",
            "246/246 [==============================] - 4s 10ms/step - loss: 178.7979\n",
            "Epoch 2/100\n",
            "246/246 [==============================] - 3s 12ms/step - loss: 0.6030\n",
            "Epoch 3/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.5494\n",
            "Epoch 4/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.8272\n",
            "Epoch 5/100\n",
            "246/246 [==============================] - 2s 10ms/step - loss: 0.6504\n",
            "Epoch 6/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.8534\n",
            "Epoch 7/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.6429\n",
            "Epoch 8/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.7027\n",
            "Epoch 9/100\n",
            "246/246 [==============================] - 2s 10ms/step - loss: 0.5748\n",
            "Epoch 10/100\n",
            "246/246 [==============================] - 3s 10ms/step - loss: 0.7226\n",
            "Epoch 11/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.5692\n",
            "Epoch 12/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.5186\n",
            "Epoch 13/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.6944\n",
            "Epoch 14/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.7620\n",
            "Epoch 15/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.6362\n",
            "Epoch 16/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.6011\n",
            "Epoch 17/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.5603\n",
            "Epoch 18/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.7963\n",
            "Epoch 19/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.5553\n",
            "Epoch 20/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.5871\n",
            "Epoch 21/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.5050\n",
            "Epoch 22/100\n",
            "246/246 [==============================] - 3s 12ms/step - loss: 0.5319\n",
            "Epoch 23/100\n",
            "246/246 [==============================] - 3s 10ms/step - loss: 0.3815\n",
            "Epoch 24/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.5963\n",
            "Epoch 25/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.5139\n",
            "Epoch 26/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.3953\n",
            "Epoch 27/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.5578\n",
            "Epoch 28/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.6688\n",
            "Epoch 29/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.5236\n",
            "Epoch 30/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.5003\n",
            "Epoch 31/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.6202\n",
            "Epoch 32/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.4909\n",
            "Epoch 33/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.6241\n",
            "Epoch 34/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.5844\n",
            "Epoch 35/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.6288\n",
            "Epoch 36/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.5087\n",
            "Epoch 37/100\n",
            "246/246 [==============================] - 3s 10ms/step - loss: 0.5463\n",
            "Epoch 38/100\n",
            "246/246 [==============================] - 3s 10ms/step - loss: 0.5151\n",
            "Epoch 39/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.5021\n",
            "Epoch 40/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.4629\n",
            "Epoch 41/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.5889\n",
            "Epoch 42/100\n",
            "246/246 [==============================] - 2s 10ms/step - loss: 0.4255\n",
            "Epoch 43/100\n",
            "246/246 [==============================] - 2s 10ms/step - loss: 0.4834\n",
            "Epoch 44/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.3931\n",
            "Epoch 45/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.4784\n",
            "Epoch 46/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.4593\n",
            "Epoch 47/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.5436\n",
            "Epoch 48/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.3973\n",
            "Epoch 49/100\n",
            "246/246 [==============================] - 2s 10ms/step - loss: 0.5580\n",
            "Epoch 50/100\n",
            "246/246 [==============================] - 2s 10ms/step - loss: 0.4408\n",
            "Epoch 51/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.3805\n",
            "Epoch 52/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.4538\n",
            "Epoch 53/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.5028\n",
            "Epoch 54/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.7891\n",
            "Epoch 55/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.4612\n",
            "Epoch 56/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.4675\n",
            "Epoch 57/100\n",
            "246/246 [==============================] - 3s 12ms/step - loss: 0.4229\n",
            "Epoch 58/100\n",
            "246/246 [==============================] - 2s 10ms/step - loss: 0.5334\n",
            "Epoch 59/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.4672\n",
            "Epoch 60/100\n",
            "246/246 [==============================] - 3s 12ms/step - loss: 0.4323\n",
            "Epoch 61/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.4271\n",
            "Epoch 62/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.4742\n",
            "Epoch 63/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.7175\n",
            "Epoch 64/100\n",
            "246/246 [==============================] - 2s 10ms/step - loss: 0.5761\n",
            "Epoch 65/100\n",
            "246/246 [==============================] - 2s 10ms/step - loss: 0.4052\n",
            "Epoch 66/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.5007\n",
            "Epoch 67/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.5159\n",
            "Epoch 68/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.4899\n",
            "Epoch 69/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.6725\n",
            "Epoch 70/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.4597\n",
            "Epoch 71/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.4253\n",
            "Epoch 72/100\n",
            "246/246 [==============================] - 2s 10ms/step - loss: 0.4813\n",
            "Epoch 73/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.4571\n",
            "Epoch 74/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.4954\n",
            "Epoch 75/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.4330\n",
            "Epoch 76/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.3669\n",
            "Epoch 77/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.4623\n",
            "Epoch 78/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.4203\n",
            "Epoch 79/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.4470\n",
            "Epoch 80/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.4320\n",
            "Epoch 81/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.4328\n",
            "Epoch 82/100\n",
            "246/246 [==============================] - 3s 12ms/step - loss: 0.4337\n",
            "Epoch 83/100\n",
            "246/246 [==============================] - 3s 12ms/step - loss: 0.4412\n",
            "Epoch 84/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.4312\n",
            "Epoch 85/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.4257\n",
            "Epoch 86/100\n",
            "246/246 [==============================] - 2s 10ms/step - loss: 0.3859\n",
            "Epoch 87/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.4387\n",
            "Epoch 88/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.4454\n",
            "Epoch 89/100\n",
            "246/246 [==============================] - 3s 11ms/step - loss: 0.3968\n",
            "Epoch 90/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.7607\n",
            "Epoch 91/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.4713\n",
            "Epoch 92/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.3774\n",
            "Epoch 93/100\n",
            "246/246 [==============================] - 2s 10ms/step - loss: 0.8074\n",
            "Epoch 94/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.4593\n",
            "Epoch 95/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.4039\n",
            "Epoch 96/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.4377\n",
            "Epoch 97/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.5098\n",
            "Epoch 98/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.3980\n",
            "Epoch 99/100\n",
            "246/246 [==============================] - 2s 8ms/step - loss: 0.4882\n",
            "Epoch 100/100\n",
            "246/246 [==============================] - 2s 9ms/step - loss: 0.4041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc00a7f5a10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_WgpGX18VeC"
      },
      "source": [
        "# demonstrate prediction\r\n",
        "x_input = pd.Series(data['Adj Close'].iloc[-4:])\r\n",
        "x_input = x_input.values.reshape((1, n_steps_in, n_features))\r\n",
        "yhat = model.predict(x_input, verbose=0)\r\n",
        "print(yhat)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}